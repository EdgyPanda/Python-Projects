---
output:
  html_document: default
  pdf_document: default
---
https://quant.stackexchange.com/questions/69999/backtesting-strategy-in-r-for-simple-empirical-value-at-risk

# Preliminaries 

The purpose of this document is to introduce a subset of different methods -- often presented in economic literature -- that are used to backtest VaR forecasts. There will be no investigation into the adequacy of the underlying models or the data. We base our VaR forecasts on the empirical VaR estimate (quantile function) against an observation-driven approach. The observation driven approach utilize a simple GARCH(1,1) framework to model the conditional variance in a parametric VaR setup, thus yielding time-varying conditional VaR estimates. These two methods will be compared using a variety of different backtesting methodologies frequently observed within the litterature:

- Failure rates
- Unconditional & Conditional coverage test
- Failure rates based on loss functions
- The Basel Committee’s heuristic traffic light test.

As implied above, the focus will only be on univariate risk modeling. 


***


### A brief note on using the empirical VaR forecasts: 

Using the empirical quantile function under a 1000-day rolling window estimation as your VaR forecasts, you're making the implicit assumption that the distribution formed by the past 1000 days of data provide reasonable VaR forecasts. This method is equivalent to the non-parametric estimation/historical simulation method. 

**The problem:** VaR estimates produced by the above methodology is fairly insensitive to large decreases in market prices (and thus negative returns) giving you only a marginal higher VaR the day after. Only after multiple consequtive days of market tumult, will the VaR estimate start to adjust better to the current data. Decreasing the window length drastically gives the opposite response: the VaR estimates become rough and a single new data point might produce drastically different VaR forecasts. In essence, the method does not allow for predictions extending beyond the extreme returns observed during the 1000-day estimation window. 

**Conclusively**, the VaR forecasts responds slowly (*or not at all*) to new information or in the case of short window-length, might become erratic and untrustworthy. It is essential that VaR forecasts reacts well to new information over shorter horizons, in order to give traders and/or portfolio managers time to downscale their positions in case of forthcoming exogenous market events. 


While all VaR methods have advantages and disadvantages, one way of circumventing the pitfalls of VaR forecasts based on historical simulation, is to use a parametric VaR framework conditioning on the filtration up until time $t$ in order to produce $h$-step ahead $VaR_{t+h}$ forecasts.


# A Parametric VaR Approach: Forecasting VaR using a simple GARCH framework

Before diving into the GARCH framework, we will briefly define the VaR as the negative (1-$\alpha$)-quantile of an arbitrary distribution:
$$
VaR^\alpha = -F_{\alpha}^{-1}(\theta), 
$$
where $\theta$ denotes the parameters of the arbitrary continuous distribution. More strictly, the VaR can be defined as: 
$$
\mathbb{P}_{\theta}\left(R_t < -VaR\right) = \alpha,
$$
or in terms of the conditional VaR, that is used under the observation-driven approach:
$$
\mathbb{P}_{\theta}\left(R_{t+1} < -VaR_{t+1\vert t} \vert \mathcal{F}_t\right) = \alpha,
$$
where $R_t$ denotes portfolio returns, $R_t = \frac{r_{t} - r_{t-1}}{r_{t-1}}$. If one is working with log-returns you can recover the VaR via the transformation:
$$
VaR^\alpha_t = e^{\text{logret VaR}_{t}^\alpha}-1. 
$$
Throughout the small study we will be working with VaR estimates based on log-returns, since we are interested in a temporal investigation of the measure and do not perform any kind of portfolio modelling. 

There are two widely different ways of defining VaR based methods in financial literature: 

- $R_t$ denotes **returns**. In this case VaR is the left-quantile ($\alpha = 0.05$) of the empirical distribution. 
- $L_t = -R_t$ are a loss process. Then VaR is the right-quantile ($1-\alpha = 0.95$) of the distribution.

Furthermore the Expected Shortfall (ES) is the expected return (loss) below (above) the VaR for the first (second) method. When working with the first notation, it is expected to get negative VaR values. In the industry (Finance and Insurance/Actuary) VaR is often denoted as dollar risk of your position/trade and therefore negating the number is common. This is done in the above definition.  


## The GARCH framework: 
Assume that we are standing at time $t$ with information $\mathcal{F}_t$, then for a constant mean model $\mu$, we can define a univariate GARCH(1,1) framework for a return process $r_{t+1}$ as: 

\begin{align}
r_t \vert \mathcal{F}_{t-1} &= \mu + \varepsilon_t\\
\varepsilon_t &= \sigma_t \cdot z_t\\
\sigma^2_t &= \alpha_0 + \alpha_1 \varepsilon_{t-1}^2 + \beta \sigma_{t-1}^2 
\end{align}
where $z_t \overset{iid}{\sim} F(0,1)$ is a distribution with mean 0 and variance 1. For example, $F(\cdot)$ could be the standardized Student's $t$ distribution with $\nu$ degrees of freedom or a simple Gaussian distribution. The parameters of the model are obtained following QMLE and the corresponding 1-step ahead VaR forecast can be derived as follows:

$$
VaR_{t\vert t-1}^\alpha = -\mu - \hat{\sigma}_{t\vert t-1} \cdot F^{-1}_{\alpha}(\hat{\theta}),
$$
where $F^{-1}_{\alpha}(\hat{\theta})$ is the $\alpha$-quantile of the distribution of $z_{t}$ with estimated parameters $\hat{\theta}$. The GARCH framework linearly scale the quantile estimates following the mean- and variance-dynamics, giving the VaR forecasts a way to react within short horizons. The flexibility of this approach lies in the specification of a mean and variance dynamics aswell as the distribution. This is, however, also one of the shortcoming of the approach: You require knowledge of the underlying distribution and a misspecification of said distribution will lead to wrong VaR forecasts. Moreover, the distribution of the innovations needs to come from a location-scale family. 

For the purpose of the analysis, I will be using the Student's $t$ distribution for the GARCH innovations. Everything have been estimated using the `rugarch` package. Furthermore, I will be focusing on 1-step ahead VaR forecasts, when illustrating the backtesting methods. 

# Backtesting VaR: Methods and applications

A way to evaluate the performance of a given VaR methodology is to perform backtesting. That is, we compare the ex-ante estimated VaR with the ex-post realized returns. There exists several different methods and tests for backtesting VaR forecasts. These are merely the methods I have observed most in academical literature. 

## Failure rates

Failure Rates is one of the simpler ways of performing a VaR backtest. The method estimate the probability of how many times the ex-post returns $r_t$ exceeds (in absolute value) the VaR forecast over the entire forecasting period. In this sense, it is based on calculating the empirical mean of an indicator function that measures the amount of VaR violations over the entire forecast horizon $T$: 
$$
\hat{f}_l = \frac{1}{T} \sum_{t=1}^{T} I_t,
$$
with
$$
I_t=\begin{cases}
1, & \text{if } \: r_{t} < - VaR_{t\vert t-1}^\alpha\\
0, & \text{if } \: r_{t} \geq - VaR_{t\vert t-1}^\alpha
\end{cases},
$$  
where $VaR_{t\vert t-1}(\alpha)$ is the 1-step ahead VaR forecast. The closer the estimated Failure Rate $\hat{f}_l$ is to the chosen coverage level $\alpha$, the more accurate the VaR model. If the number of total ex-post losses exceeds $\alpha$\%, then the VaR model is inaccurate and underestimates the forthcoming risk. Likewise, the model can be seen as too conservative if the percentage of exceedances are less than the $\alpha$\% and thus overestimates the risk (*this is not a major issue*). The adequacy of the VaR model tested by whether the Failure Rates equates the chosen coverage level $\alpha$, can be theoretically reformulated via the conditional expectation:
$$
\mathbb{E}\left[I_t \vert \mathcal{F}_{t-1}\right] = \alpha, \label{1}
$$
which is also called the conditional coverage level for the VaR model. Alleviating the conditioning of the filtration, gives us the *unconditional coverage level*. Many of the VaR evaluation tests proposed in literature assess the adequacy of VaR
predictions by testing against different variations of the above equation.  


## Unconditional and Conditional Coverage tests
Expanding the filtration of the conditional coverage level $\mathcal{F}_{t-1} = \{I_1,\ldots,I_{t-1}\}$ it can be shown (as originally seen in the original paper of [Christoffersen (1998)](https://www.jstor.org/stable/2527341?casa_token=G2yzQO64_NcAAAAA%3Aw4S_Qf72xDK_eeDZoifvAzX6gmFZLo4Gba6-IN3GnGeMd9PXc4vTDilyUqr0FxqqThtzdSVI171UEEHu2KUA29_ZmMM5-6QNTT4MfTE2nw1k7RjAlkOE&seq=1) lemma 1) that the distribution of the conditional Failure Rates is *i.i.d* Bernoulli distributed with parameter $\alpha$:

$$
I_t \vert \mathcal{F}_{t-1} \overset{i.i.d}{\sim} Ber(\alpha).
$$

Taking iterated expectations then the formula (1) implies correct unconditional coverage of the interval forecasts. We test for correct number of violations via: 

$$
H_0: \:\: \mathbb{E}\left[I_t\right] = \alpha, \qquad H_a:  \:\: \mathbb{E}\left[I_t\right] \neq \alpha.
$$

The Likelihood Ratio (LR) statistic for verifying that $\hat{\alpha} = \alpha$ is given below, 

$$
LR_{uc} = 2 \cdot \left(\ln\left[(1-\hat{\alpha})^{n_0} \cdot \hat{\alpha}^{n_1}\right] - \ln\left[(1-\alpha)^{n_0} \cdot \alpha^{n_1}\right] \right) \overset{a}{\sim} \chi^2(1),
$$
with $n_0 = n - n_1$ and $n_1 = \sum_{i=1}^n I_i$ be the number of exceptions. Here, $\hat{\alpha} = \frac{n_1}{n}$. The null is rejected in the cases where the VaR provides too few or too many exceptions. 

**The problem** with this test statistic is the indepencence of the failure process, $I_t$. That is, the test statistic does not take into account the order of zeroes and ones, it only pays attention to the total amount of exceptions. The failure process tends to cluster in conjunction with the volatility clustering of the underlying asset. The failure rate can also cluster around time intervals where the VaR forecasts severely underestimates the forthcoming risk. 

This gives rise to the conditional coverage test of Christoffersen (1998) where the null expects the exceptions to be independently distributed through time. The test statistic can be represented in the following way (*see Louzis et al. (2011)*):

$$
LR_{cc} = 2 \left(  \ln\left[(1-\hat{\alpha}_{01})^{n_{00}} 
\hat{\alpha}_{01}^{n_{01}} (1-\hat{\alpha}_{11})^{n_{10}} \hat{\alpha}_{11}^{n_{11}}\right] - \ln\left[ (1-\alpha)^{n_0} \alpha^{n_1}\right] \right) \overset{a}{\sim} \chi^2(2),
$$ 
where $\alpha_{ij}$ denotes the transition probability between two consecutive observations from state *i* to state *j* assuming a first-order Markov chain probability transition matrix between two states (*successfull VaR estimation or an exception*), $n_{ij}$ is the number of all occcurences of transitions from state *i* to state *j* and $\hat{\alpha}_{ij} = \frac{n_{ij}}{\sum_{j=0}^1 n_{ij}}$ are the maximum likelihood estimates for $\alpha_{ij}$. The test can be seen as two tests $LR_{cc} = LR_{uc} + LR_{ind}$ where an accurate VaR model must display both the unconditional coverage property and independence of violations. 

This test has a reduced ability to detect a VaR models which only violate one of the two properties. If one of the two properties are satisfied, the test statistic finds it more difficult to detect the inadequacy of the VaR models. 

## Failure rates based on loss functions

While the above methods are good for testing the adequacy of various VaR model, they do not provide a way to directly compare the *accuracy* of each VaR model, which is a matter of regulatory concern. In the paper of [Lopez (1998)](https://www.newyorkfed.org/research/economists/medialibrary/media/research/staff_reports/research_papers/9802.pdf) he proposes to combine Failure Rates with loss functions, specifically the quadratic loss function (QLF). The general form is given by:

$$
L = \frac{1}{T} \sum_{t=1}^{T} V_t
$$ 
with 
$$
V_t=\begin{cases}
1+\left(r_t+ VaR_{t\vert t-1}^\alpha\right)^2, & \text{if } \: r_t < -VaR_{t\vert t-1}^\alpha\\
0, & \text{if} \: r_t \geq -VaR_{t \vert t-1}^\alpha
\end{cases},
$$
where $V_t$ is the QLF, which considers both the number of exceptions/violations and the magnitude as the square between the ex-post returns and the VaR forecast. The model with the smallest average QLF is considered to be the most accurate. However it needs to be noted that the methodology favours conservative models as their violations are below the user-defined $\alpha$-level.  


## The Basel Committee’s heuristic traffic light test

The Traffic light test described in a [supervisory framework](https://www.bis.org/publ/bcbs22.pdf) from the Basel Comittee was proposed in early 1996 with the purpose of providing a heuristic backtesting methodology to be used within financial institutions in order to test the adequacy of their internal VaR models.

Let $x_t = \sum_{t=1}^{250} I_t$ be the number of violations under a 250 day forecasting period. Then the adequacy of the financial institutions internal VaR model can be assessed by the heuristic piece-wise function: 
$$
TL = \begin{cases}
\text{green}, & \text{if } \: x_t \leq 4\\
\text{yellow}, & \text{if } \: 5 \leq x_t \leq 9\\
\text{red}, & \text{if } \: 10 \leq x_t 
\end{cases},
$$
where green indicates that the VaR model is to be assumed accurate, yellow is neither nor, and red indicates that the model is assumed inaccurate and needs a revision. In the original paper, these three are referred to as the green zone, yellow zone and red zone, respectively. This test is very simple and is best used as a preliminary check for the adequacy of the VaR forecasts. 


# Empirical analysis and Conclusion

present empirics, graphs and conclusions.

Part with stressed VaR (under stressed scenarios --> Financial Crisis etc.). 



# Additional literature

talk about high-frecuency literature and how it can improve the estimates of 
VaR based forecasts. link to maybe some of pattons articles? 

As referrenced above, there are additional VaR backtesting methods not covered above. I have provided links to a paper detailing additional backtesting methodologies: 

Zhang, Y., & Nadarajah, S. (2018). [*A review of backtesting for value at risk.*](https://www.tandfonline.com/doi/full/10.1080/03610926.2017.1361984?casa_token=suSAAFqZj6QAAAAA%3AIG4CopQJoseHUq7gN_BdQR4z4_aQLRHgEgFJInQti9vG3z1Ep_NTy2gau879n7rlQ3Ziumdbq30UeQ)Communications in Statistics-Theory and methods, 47(15), 3616-3639.

# References



Lopez, J. A. (1998). [*Methods for evaluating value-at-risk estimates*](https://www.newyorkfed.org/research/economists/medialibrary/media/research/staff_reports/research_papers/9802.pdf). Economic Policy Review, 4(3).

Zhang, Y., & Nadarajah, S. (2018). [*A review of backtesting for value at risk.*](https://www.tandfonline.com/doi/full/10.1080/03610926.2017.1361984?casa_token=suSAAFqZj6QAAAAA%3AIG4CopQJoseHUq7gN_BdQR4z4_aQLRHgEgFJInQti9vG3z1Ep_NTy2gau879n7rlQ3Ziumdbq30UeQ)Communications in Statistics-Theory and methods, 47(15), 3616-3639.


Giot, P., & Laurent, S. (2004). *Modelling daily value-at-risk using realized volatility and ARCH type models.*(https://www.sciencedirect.com/science/article/pii/S092753980400012X?casa_token=PzIdUuE8lYsAAAAA:8M9kLjXCgNTwrrdgQyGptpjADEFWxpD2XhXi2R83sl88RG0-P34DlMHFaLCt5N4uoJGuRRQ550A) Journal of empirical finance, 11(3), 379-398.


Basel Committee of Banking Supervision (1996). [*Supervisory Framework for the Use
of “Backtesting” in Conjunction with the Internal Models Approach to Market Risk
Capital Requirements*](https://www.bis.org/publ/bcbs22.pdf). 

Kuester, K., Mittnik, S., & Paolella, M. S. (2006). [*Value-at-risk prediction: A comparison of alternative strategies*](https://academic.oup.com/jfec/article-abstract/4/1/53/833052?login=false). Journal of Financial Econometrics, 4(1), 53-89.

Sheppard, K. (2021). [*Financial Econometrics Notes*](https://www.kevinsheppard.com/files/teaching/mfe/notes/financial-econometrics-2020-2021.pdf)

Christoffersen, P. F. (1998). [*Evaluating interval forecasts*](https://www.jstor.org/stable/2527341?casa_token=G2yzQO64_NcAAAAA%3Aw4S_Qf72xDK_eeDZoifvAzX6gmFZLo4Gba6-IN3GnGeMd9PXc4vTDilyUqr0FxqqThtzdSVI171UEEHu2KUA29_ZmMM5-6QNTT4MfTE2nw1k7RjAlkOE&seq=1). International economic review, 841-862.

Louzis, D. P., Xanthopoulos-Sisinis, S., & Refenes, A. N. (2011). *Are realized volatility models good candidates for alternative Value at Risk prediction strategies?.*(https://mpra.ub.uni-muenchen.de/30364/1/MPRA_paper_30364.pdf) Available at SSRN 1814171.